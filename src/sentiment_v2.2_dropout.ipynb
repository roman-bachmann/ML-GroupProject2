{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Classification - Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import gensim\n",
    "import Cython\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from torchtext import data\n",
    "\n",
    "from helpers import *\n",
    "from data import create_csv_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../twitter-datasets/'\n",
    "MODEL_PATH = '../models/'\n",
    "\n",
    "TRAIN_NEG_PATH = os.path.join(DATA_PATH, 'train_neg.txt') # 100'000 negative tweets\n",
    "TRAIN_POS_PATH = os.path.join(DATA_PATH, 'train_pos.txt') # 100'000 positive tweets\n",
    "#TRAIN_NEG_PATH = os.path.join(DATA_PATH, 'train_neg_full.txt') # 2'500'000 negative tweets\n",
    "#TRAIN_POS_PATH = os.path.join(DATA_PATH, 'train_pos_full.txt') # 2'500'000 positive tweets\n",
    "TEST_PATH = os.path.join(DATA_PATH, 'test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text_train, y_train_full = load_data_and_labels(TRAIN_POS_PATH, TRAIN_NEG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text_test = load_test_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build word2vec vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(x_text_train + x_text_test, min_count=1, workers=4, size=vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model.save(MODEL_PATH + 'twitter_w2v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec.load(MODEL_PATH + 'twitter_w2v.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When training finished delete the training model but retain the word vectors:\n",
    "word_vectors = w2v_model.wv\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.wv['computer'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('internet', 0.708969235420227),\n",
       " ('desktop', 0.6651554107666016),\n",
       " ('calculator', 0.6596795320510864),\n",
       " ('settings', 0.6567734479904175),\n",
       " ('laptop', 0.6565127372741699),\n",
       " ('phone', 0.6472017168998718),\n",
       " ('browser', 0.635584831237793),\n",
       " ('portfolio', 0.6346469521522522),\n",
       " ('wifi', 0.6323057413101196),\n",
       " ('desk', 0.6210508346557617)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar('computer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert tweets into sentences of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length of train and test data: 50\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of words of the longest tweet to get the maximal sentence length\n",
    "sequence_length_train = max(len(x) for x in x_text_train)\n",
    "sequence_length_test = max(len(x) for x in x_text_test)\n",
    "sequence_length = max(sequence_length_train, sequence_length_test)\n",
    "print('Maximum sequence length of train and test data:', sequence_length)\n",
    "\n",
    "x_text_train_pad = pad_sentences(x_text_train, padding_word=\"<PAD/>\", sequence_length=sequence_length)\n",
    "x_text_test_pad = pad_sentences(x_text_test, padding_word=\"<PAD/>\", sequence_length=sequence_length)\n",
    "\n",
    "del x_text_train\n",
    "del x_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into training and validation data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_text_train_pad, y_train_full, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198000, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_val), len(x_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: SLOW!\n",
    "\n",
    "def get_tweets_tensor(tweets, indices=[], verbose=False):\n",
    "    '''Mapping every word to a vector from word2vec\n",
    "    Padding words are mapped to zero\n",
    "    Leave indices empty to map every tweet in tweets\n",
    "    '''\n",
    "\n",
    "    nb_tweets = len(tweets) if len(indices)==0 else len(indices)\n",
    "    tweets_vec = np.zeros((nb_tweets, len(tweets[0]), vector_length), dtype=np.float32)\n",
    "    \n",
    "    if indices == []:\n",
    "        for idx_t, tweet in enumerate(tweets):\n",
    "            for idx_w, word in enumerate(tweet):\n",
    "                if word != '<PAD/>':\n",
    "                    tweets_vec[idx_t, idx_w] = word_vectors.wv[word]  \n",
    "            if verbose and idx_t % 100000 == 0:\n",
    "                print('Transformed {}/{} tweets'.format(idx_t+1, (len(x_text_train_pad))))\n",
    "    else:\n",
    "        for idx_t, orig_idx in enumerate(indices):\n",
    "            for idx_w, word in enumerate(tweets[orig_idx]):\n",
    "                if word != '<PAD/>':\n",
    "                    tweets_vec[idx_t, idx_w] = word_vectors.wv[word]  \n",
    "            if verbose and idx_t % 100000 == 0:\n",
    "                print('Transformed {}/{} tweets'.format(idx_t+1, (len(x_text_train_pad))))\n",
    "    \n",
    "    return torch.from_numpy(tweets_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "num_epochs = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping data and target lists.\n",
    "\n",
    "    Each sample will be retrieved by indexing both lists along the first\n",
    "    dimension.\n",
    "\n",
    "    Arguments:\n",
    "        data_list (python list): contains sample data.\n",
    "        target_list (python list): contains sample targets (labels).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_list, target_list):\n",
    "        assert len(data_list) == len(target_list)\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_list[index], self.target_list[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = ListDataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network hyperparameters\n",
    "N = len(train_loader.dataset.data_list)      # Number of tweets (eg 200000)\n",
    "S = len(train_loader.dataset.data_list[0])   # Number of words in one sentence (eg 50)\n",
    "V = vector_length                            # Length of word vectors (eg 100)\n",
    "K = 3                                        # Kernel width (K*V)\n",
    "C = 128                                      # Number of convolutional filters\n",
    "F = 2                                        # Number of output neurons in fully connected layer\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(             # input shape (1, S, V)\n",
    "                in_channels=1,              # input channels\n",
    "                out_channels=C,             # number of filters\n",
    "                kernel_size=(K,V),          # filter size\n",
    "                padding=(1,0)               # to keep size S\n",
    "        )                                   # output shape (C, S, 1)\n",
    "        self.relu = nn.ReLU()               # ReLU activation\n",
    "        self.max_pool1 = nn.MaxPool1d(2)    # max-pool each filter into S/2 output   \n",
    "        self.conv2 = nn.Conv1d(             # input shape (C, S/2)\n",
    "                in_channels=C,              # input channels\n",
    "                out_channels=C,             # number of filters (one for each input channel)\n",
    "                kernel_size=K,              # filter size\n",
    "                padding=1                   # to keep size S/2\n",
    "        )                                   # output shape (C, S/2)\n",
    "        self.max_pool2 = nn.MaxPool1d(int(S/2)) # max pool each filter into 1 output\n",
    "        self.dropout= nn.Dropout(p=0.2)\n",
    "        self.out = nn.Linear(C, F)          # fully connected layer, output F classes\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.data.shape) #torch.Size([100, 74, 100])\n",
    "        \n",
    "        out = x.unsqueeze(1)\n",
    "        #print(out.data.shape) #torch.Size([100, 1, 74, 100])\n",
    "        \n",
    "        out = self.conv1(out)\n",
    "        #print(out.data.shape) #torch.Size([100, 128, 74, 1])\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.relu(out).squeeze(3)\n",
    "        #print(out.data.shape) #torch.Size([100, 128, 74])\n",
    "        \n",
    "        out = self.max_pool1(out)\n",
    "        #print(out.data.shape) #torch.Size([100, 128, 37])\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        #print(out.data.shape) #torch.Size([100, 128, 37])\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        #print(out.data.shape) #torch.Size([100, 128, 37])\n",
    "        \n",
    "        out = self.max_pool2(out).squeeze(2).float()\n",
    "        #print(out.data.shape) #torch.Size([100, 128])\n",
    "        \n",
    "        out = self.out(out)\n",
    "        #print(out.data.shape) #torch.Size([100, 2])\n",
    "        \n",
    "        #out = self.softmax(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (conv1): Conv2d(1, 128, kernel_size=(3, 100), stride=(1, 1), padding=(1, 0))\n",
      "  (relu): ReLU ()\n",
      "  (max_pool1): MaxPool1d (size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (max_pool2): MaxPool1d (size=25, stride=25, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout (p = 0.2)\n",
      "  (out): Linear (128 -> 2)\n",
      "  (softmax): Softmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                                  # the target label is not one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Iter [100/1980] Loss: 0.3332\n",
      "Epoch [1/1], Iter [200/1980] Loss: 0.3666\n",
      "Epoch [1/1], Iter [300/1980] Loss: 0.2613\n",
      "Epoch [1/1], Iter [400/1980] Loss: 0.2860\n",
      "Epoch [1/1], Iter [500/1980] Loss: 0.2895\n",
      "Epoch [1/1], Iter [600/1980] Loss: 0.3087\n",
      "Epoch [1/1], Iter [700/1980] Loss: 0.3230\n",
      "Epoch [1/1], Iter [800/1980] Loss: 0.2354\n",
      "Epoch [1/1], Iter [900/1980] Loss: 0.3294\n",
      "Epoch [1/1], Iter [1000/1980] Loss: 0.3176\n",
      "Epoch [1/1], Iter [1100/1980] Loss: 0.3953\n",
      "Epoch [1/1], Iter [1200/1980] Loss: 0.4779\n",
      "Epoch [1/1], Iter [1300/1980] Loss: 0.2893\n",
      "Epoch [1/1], Iter [1400/1980] Loss: 0.2955\n",
      "Epoch [1/1], Iter [1500/1980] Loss: 0.3214\n",
      "Epoch [1/1], Iter [1600/1980] Loss: 0.3992\n",
      "Epoch [1/1], Iter [1700/1980] Loss: 0.3322\n",
      "Epoch [1/1], Iter [1800/1980] Loss: 0.3271\n",
      "Epoch [1/1], Iter [1900/1980] Loss: 0.3448\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times \n",
    "    for i, batch_indices in enumerate(train_loader.batch_sampler):   # iterate over mini-batches  \n",
    "        # Converting tweets to vectors and storing it in a variable\n",
    "        sentences = get_tweets_tensor(train_loader.dataset.data_list, batch_indices, verbose=False)\n",
    "        x = Variable(sentences)\n",
    "        \n",
    "        # Converting labels to a variable\n",
    "        labels = torch.from_numpy(train_loader.dataset.target_list[batch_indices])\n",
    "        y = Variable(labels, requires_grad=False)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad() # reset gradient\n",
    "        outputs = cnn(x) # cnn output\n",
    "        loss = loss_func(outputs, y) # clear gradients for this training step\n",
    "        loss.backward() # backpropagation, compute gradients\n",
    "        optimizer.step() # apply gradients\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                  %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8325\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy of predictions from validation data\n",
    "val_output = cnn(Variable(get_tweets_tensor(x_val)))\n",
    "y_val_pred = torch.max(val_output, 1)[1].data.numpy().squeeze()\n",
    "\n",
    "print('Validation accuracy:', accuracy_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Make predictions for test data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_output = cnn(Variable(get_tweets_tensor(x_text_test_pad, verbose=False)))\n",
    "y_pred = torch.max(test_output, 1)[1].data.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_pred == 0] = -1\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.arange(len(y_pred)+1)[1:]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids, y_pred, 'kaggle_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
