{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np \n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/lia/work/study/epfl/CS433-MachineLearning/ML-GroupProject2/twitter-datasets'\n",
    "\n",
    "TRAIN_NEG_PATH = os.path.join(DATA_PATH, 'train_neg.txt') # 100'000 negative tweets\n",
    "TRAIN_POS_PATH = os.path.join(DATA_PATH, 'train_pos.txt') # 100'000 positive tweets\n",
    "# TRAIN_NEG_PATH = os.path.join(DATA_PATH, 'train_neg_full.txt') # 2'500'000 negative tweets\n",
    "# TRAIN_POS_PATH = os.path.join(DATA_PATH, 'train_pos_full.txt') # 2'500'000 positive tweets\n",
    "# TEST_PATH = os.path.join(DATA_PATH, 'test_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(train_positive_path, train_negative_path):\n",
    "    \"\"\"\n",
    "    Loads tweet data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    positive_examples = list(open(train_positive_path).readlines())\n",
    "    negative_examples = list(open(train_negative_path).readlines())\n",
    "    x_text_train = positive_examples + negative_examples\n",
    "    \n",
    "    x_tokens_train = [word_tokenize(x) for x in x_text_train]\n",
    "    \n",
    "    positive_labels = [1 for _ in positive_examples]\n",
    "    negative_labels = [0 for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text_train, x_tokens_train, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_text_train, x_tokens_train, y_train_full = load_data_and_labels(TRAIN_POS_PATH, TRAIN_NEG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_tokens_train,\n",
    "                                                    y_train_full, \n",
    "                                                    test_size=0.01, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198000it [00:01, 161588.48it/s]\n",
      "2000it [00:00, 310493.69it/s]\n"
     ]
    }
   ],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledSentence(words=['<', 'user', '>', 'hey', 'thankx', 'for', 'following', 'dear'], tags=['TRAIN_0'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198000/198000 [00:00<00:00, 1950467.94it/s]\n",
      "100%|██████████| 198000/198000 [00:00<00:00, 1927739.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 107666\n"
     ]
    }
   ],
   "source": [
    "tweet_w2v = Word2Vec(size=n_dim, min_count=1)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(x_train)])\n",
    "tweet_w2v.train([x.words for x in tqdm(x_train)], \n",
    "                epochs=tweet_w2v.iter,\n",
    "                total_examples=tweet_w2v.corpus_count)\n",
    "print('Number of words: {}'.format(len(tweet_w2v.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nice', 0.7896394729614258),\n",
       " ('great', 0.7598593235015869),\n",
       " ('bad', 0.7344122529029846),\n",
       " ('lovely', 0.6982096433639526),\n",
       " ('terrible', 0.6812501549720764),\n",
       " ('ah.bad', 0.6438660621643066),\n",
       " ('busy', 0.6360503435134888),\n",
       " ('awesome', 0.6346652507781982),\n",
       " ('goood', 0.6319184303283691),\n",
       " ('horrible', 0.6308711767196655)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # importing bokeh library for interactive dataviz\n",
    "# import bokeh.plotting as bp\n",
    "# from bokeh.models import HoverTool, BoxSelectTool\n",
    "# from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "# # defining the chart\n",
    "# output_notebook()\n",
    "# plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"A map of 10000 word vectors\",\n",
    "#     tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "#     x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "# # getting a list of word vectors. limit to 10000. each is of 200 dimensions\n",
    "# word_vectors = [tweet_w2v[w] for w in list(tweet_w2v.wv.vocab.keys())[:5000]]\n",
    "\n",
    "# # dimensionality reduction. converting the vectors to 2d vectors\n",
    "# from sklearn.manifold import TSNE\n",
    "# tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "# tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "\n",
    "# # putting everything in a dataframe\n",
    "# tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "# tsne_df['words'] = list(tweet_w2v.wv.vocab.keys())[:5000]\n",
    "\n",
    "# # plotting. the corresponding word appears when you hover on the data point.\n",
    "# plot_tfidf.scatter(x='x', y='y', source=tsne_df)\n",
    "# hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "# hover.tooltips={\"word\": \"@words\"}\n",
    "# show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 11884\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print('vocab size : {}'.format(len(tfidf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198000it [00:25, 7744.36it/s]\n",
      "2000it [00:00, 7909.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 2s - loss: 0.4707 - acc: 0.7639\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.4418 - acc: 0.7840\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.4363 - acc: 0.7878\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.4324 - acc: 0.7908\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.4289 - acc: 0.7932\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.4274 - acc: 0.7951\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.4259 - acc: 0.7963\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.4237 - acc: 0.7969\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.4238 - acc: 0.7976\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.4222 - acc: 0.7984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146dbd898>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Convolution1D, Flatten, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=10, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805499999523\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
